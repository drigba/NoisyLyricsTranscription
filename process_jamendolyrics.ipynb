{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed9e11a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ed9e11a",
    "outputId": "37001d16-0340-4b4e-d6e0-4964e7a7116b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m501.8/542.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.2/289.2 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install datasets --quiet\n",
    "!pip install accelerate -U --quiet\n",
    "!pip install evaluate --quiet\n",
    "!pip install jiwer -U  -q\n",
    "!pip install transformers -U -q\n",
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a85e70",
   "metadata": {
    "id": "b1a85e70"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/f90/jamendolyrics.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "OhjulrrcPkKl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhjulrrcPkKl",
    "outputId": "f4009eb3-51ab-4804-850f-f7a979e23711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=13bUduvbSf4AowpM5NN-owNKUYdQsrfm_\n",
      "From (redirected): https://drive.google.com/uc?id=13bUduvbSf4AowpM5NN-owNKUYdQsrfm_&confirm=t&uuid=c7edd121-a586-4044-9c24-2cd060cc09cc\n",
      "To: /content/musan_resampled.zip\n",
      "100% 1.61G/1.61G [00:07<00:00, 222MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 13bUduvbSf4AowpM5NN-owNKUYdQsrfm_\n",
    "!unzip -q /content/musan_resampled.zip\n",
    "!rm /content/musan_resampled.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "LzuC3YKrkKcH",
   "metadata": {
    "collapsed": true,
    "id": "LzuC3YKrkKcH"
   },
   "outputs": [],
   "source": [
    "# !wget https://www.openslr.org/resources/17/musan.tar.gz\n",
    "# !tar -zxf musan.tar.gz\n",
    "# !rm -rf /content/musan/music\n",
    "# !rm -rf /content/musan/speech\n",
    "# !rm /content/musan.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sXpWUXwgBn2E",
   "metadata": {
    "id": "sXpWUXwgBn2E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from datasets import DatasetDict, Audio, load_from_disk, concatenate_datasets\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torchaudio\n",
    "import soundfile\n",
    "from torch.utils.data import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "VAfD3Cn8HQuc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAfD3Cn8HQuc",
    "outputId": "54358187-a700-46ca-a905-901ab8c23d7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbanfizsombor1999\u001b[0m (\u001b[33mdrigba\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "GZhJijLABzUr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "GZhJijLABzUr",
    "outputId": "348caf2f-d936-4467-bcbb-bacde6cef3fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"openai/whisper-medium\"\n",
    "LANGUAGE = \"en\"\n",
    "FREEZE_FEATURE_ENCODER = False\n",
    "FREEZE_ENCODER = False\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=LANGUAGE, task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hc0w1TYVKO4X",
   "metadata": {
    "id": "hc0w1TYVKO4X"
   },
   "outputs": [],
   "source": [
    "if FREEZE_FEATURE_ENCODER:\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "if FREEZE_ENCODER:\n",
    "    model.freeze_encoder()\n",
    "    model.model.encoder.gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc43dd3",
   "metadata": {
    "id": "cbc43dd3"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "import librosa\n",
    "\n",
    "SR = 16000\n",
    "mp3_path = '/content/jamendolyrics/mp3/'\n",
    "csv_path = '/content/jamendolyrics/annotations/lines/'\n",
    "file_names = sorted(glob.glob(csv_path + '*.csv'))\n",
    "\n",
    "gather_n = 4 ################################################### Combine n data samples into one data\n",
    "len_files = []\n",
    "start_time = []\n",
    "end_time = []\n",
    "audio_types = []\n",
    "lyrics = []\n",
    "audio_dict = {} # load audio array in advance to use less RAM. key: 0~78, value: np.array\n",
    "\n",
    "for f in file_names:\n",
    "    c = pd.read_csv(f)\n",
    "    len_files.append(len(c))\n",
    "\n",
    "def preprocess_dataset():\n",
    "    for num, filename in enumerate(file_names):\n",
    "        len_csv = len_files[num]\n",
    "        with open(f\"{filename}\", \"r\") as f:\n",
    "            csv_data = csv.reader(f)\n",
    "            audio_name = filename.split('/')[-1][:-4] + '.mp3'\n",
    "\n",
    "            # print(f'{num}th csv file (filename: {audio_name})')\n",
    "            for i, line in enumerate(csv_data):\n",
    "                if i== 0: # except first head\n",
    "                    continue\n",
    "\n",
    "                if gather_n == 1: # not combine samples\n",
    "                    st = float(line[0])\n",
    "                    et = float(line[1])\n",
    "                    lyric = line[2]\n",
    "                    lyric = processor.tokenizer(lyric).input_ids\n",
    "\n",
    "                    start_time.append(st)\n",
    "                    end_time.append(et)\n",
    "                    lyrics.append(lyric)\n",
    "                    audio_types.append(num)\n",
    "\n",
    "                else:\n",
    "                    if i%gather_n == 1: # start gathering frames\n",
    "                        st = float(line[0])\n",
    "                        et = float(line[1])\n",
    "                        lyric = line[2]\n",
    "\n",
    "                    else:\n",
    "                        et = float(line[1])\n",
    "                        lyric += (' ' + line[2])\n",
    "\n",
    "                    if i%gather_n == 0 or i == len_csv: # end of the group or the last line\n",
    "                        start_time.append(st)\n",
    "                        end_time.append(et)\n",
    "                        lyric = processor.tokenizer(lyric).input_ids\n",
    "                        lyrics.append(lyric)\n",
    "                        audio_types.append(num)\n",
    "\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86360ee4",
   "metadata": {
    "collapsed": true,
    "id": "86360ee4"
   },
   "outputs": [],
   "source": [
    "preprocess_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ea96c2",
   "metadata": {
    "id": "f7ea96c2"
   },
   "outputs": [],
   "source": [
    "# Load original audio arrays in advance\n",
    "for i, filename in enumerate(file_names):\n",
    "    audio_name = filename.split('/')[-1][:-4] + '.mp3'\n",
    "    audio_dict[i], _ = librosa.load(mp3_path + audio_name, sr=SR)\n",
    "\n",
    "trimmed_audios = []\n",
    "def trim_audio(audio_name, st, et):\n",
    "    start_frame = SR*st\n",
    "    end_frame = SR*et\n",
    "    audio  = audio_dict[audio_name] # sr=44100\n",
    "    trimmed_audio = audio[int(start_frame):int(end_frame)+1]\n",
    "    # trimmed_audio_processed = processor.feature_extractor(trimmed_audio, sampling_rate=SR).input_features[0]\n",
    "\n",
    "    return trimmed_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "147f1351",
   "metadata": {
    "id": "147f1351"
   },
   "outputs": [],
   "source": [
    "for audio_type, st, et in zip(audio_types, start_time, end_time):\n",
    "    trimmed_audios.append(trim_audio(audio_type, st, et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lD1sCie3j02I",
   "metadata": {
    "id": "lD1sCie3j02I"
   },
   "outputs": [],
   "source": [
    "# noise_ds = load_dataset('./musan')\n",
    "noise_ds = load_from_disk('/content/content/musan_resampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cabf3f19",
   "metadata": {
    "id": "cabf3f19"
   },
   "outputs": [],
   "source": [
    "# Pytorch Dataset\n",
    "# Have to add noise before feature extraction\n",
    "\n",
    "\n",
    "def add_noise_to_audio(audio, noise, clean_db, noise_snr):\n",
    "    noise_db = 10 * np.log10(np.mean(noise ** 2)+1e-4)\n",
    "    noises = [np.sqrt(10 ** ((clean_db - noise_db - noise_snr) / 10)) * noise]\n",
    "\n",
    "    if len(noises[0]) < len(audio):\n",
    "        added = np.pad(noises[0], (0, len(audio)-len(noises[0])), 'constant') + audio\n",
    "    else:\n",
    "        added = audio + noises[0][:len(audio)]\n",
    "\n",
    "    return added\n",
    "\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, trimmed_audios, lyrics,noise_ds,random_augment = True):\n",
    "        self.lyrics = lyrics\n",
    "        self.trimmed_audios = trimmed_audios\n",
    "        self.noise_dataset = noise_ds\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        self.random_augment = random_augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lyrics)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.random_augment:\n",
    "          noise_index = random.randint(0, len(self.noise_dataset) - 1)\n",
    "        else:\n",
    "          noise_index = index % len(self.noise_dataset)\n",
    "\n",
    "        noise_sample = self.noise_dataset[noise_index]\n",
    "        noise_array = noise_sample['audio']['array'] * 30\n",
    "        audio_array = self.trimmed_audios[index]\n",
    "\n",
    "        clean_db = 10 * np.log10(np.mean(audio_array ** 2)+1e-4) ##\n",
    "        audio_array = audio_array.squeeze()#.numpy()\n",
    "        noisy_audio = add_noise_to_audio(audio_array, noise_array, clean_db, noise_snr=10)\n",
    "\n",
    "        audio_processed = self.feature_extractor(noisy_audio, sampling_rate=SR).input_features[0]\n",
    "\n",
    "        return {'raw_audio':noisy_audio , 'input_features': audio_processed, 'labels': self.lyrics[index]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48288eb8",
   "metadata": {
    "id": "48288eb8"
   },
   "outputs": [],
   "source": [
    "paired_dataset = PairedDataset(trimmed_audios, lyrics, noise_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d475f",
   "metadata": {
    "id": "072d475f"
   },
   "source": [
    "### Approach 1. Use pytorch dataset format (I'm not sure if this format is also available in huggingface Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48d72369",
   "metadata": {
    "id": "48d72369"
   },
   "outputs": [],
   "source": [
    "### Approach 1. Use pytorch dataset format (I'm not sure if this format is also available in huggingface Trainer)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "len_full_dataset = len(paired_dataset)\n",
    "\n",
    "# train samples\n",
    "train_p = 0.8\n",
    "len_train = int(len_full_dataset * train_p)\n",
    "\n",
    "# valid samples\n",
    "valid_p = 0.1\n",
    "len_valid = int(len_full_dataset * 0.1)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(paired_dataset, [len_train, len_full_dataset-len_train])\n",
    "valid_dataset, test_dataset = random_split(valid_dataset, [len_valid, len(valid_dataset)-len_valid])\n",
    "\n",
    "train_dataset.random_augment = True\n",
    "valid_dataset.random_augment = False\n",
    "test_dataset.random_augment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda7d1ab",
   "metadata": {
    "id": "fda7d1ab"
   },
   "outputs": [],
   "source": [
    "# # Check if the audio aligns with text well\n",
    "# import IPython.display as ipd\n",
    "# idx = 20\n",
    "# print(train_dataset[idx]['labels'])\n",
    "# ipd.Audio(train_dataset[idx]['input_features'], rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "TGlE7CG5vsrT",
   "metadata": {
    "id": "TGlE7CG5vsrT"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\", padding=True)\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "uibPFbArvy23",
   "metadata": {
    "id": "uibPFbArvy23"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0EmVp_4wOQgQ",
   "metadata": {
    "id": "0EmVp_4wOQgQ"
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")\n",
    "DO_NORMALIZE_EVAL = True\n",
    "normalizer = BasicTextNormalizer()\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    if DO_NORMALIZE_EVAL:\n",
    "        pred_str = [normalizer(pred) for pred in pred_str]\n",
    "        label_str = [normalizer(label) for label in label_str]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "FKZ9X2YeW5-H",
   "metadata": {
    "id": "FKZ9X2YeW5-H"
   },
   "outputs": [],
   "source": [
    "from transformers.integrations import WandbCallback\n",
    "\n",
    "\n",
    "def decode_predictions(tokenizer, predictions):\n",
    "    pred_ids = predictions.predictions\n",
    "    label_ids = predictions.label_ids\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    return {\"labels\": label_str, \"predictions\": pred_str}\n",
    "\n",
    "\n",
    "class WandbPredictionProgressCallback(WandbCallback):\n",
    "    \"\"\"Custom WandbCallback to log model predictions during training.\n",
    "\n",
    "    This callback logs model predictions and labels to a wandb.Table at each logging step during training.\n",
    "    It allows to visualize the model predictions as the training progresses.\n",
    "\n",
    "    Attributes:\n",
    "        trainer (Trainer): The Hugging Face Trainer instance.\n",
    "        tokenizer (AutoTokenizer): The tokenizer associated with the model.\n",
    "        sample_dataset (Dataset): A subset of the validation dataset for generating predictions.\n",
    "        num_samples (int, optional): Number of samples to select from the validation dataset for generating predictions. Defaults to 100.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainer, tokenizer, val_dataset, num_samples=100, freq=2):\n",
    "        \"\"\"Initializes the WandbPredictionProgressCallback instance.\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer): The Hugging Face Trainer instance.\n",
    "            tokenizer (AutoTokenizer): The tokenizer associated with the model.\n",
    "            val_dataset (Dataset): The validation dataset.\n",
    "            num_samples (int, optional): Number of samples to select from the validation dataset for generating predictions. Defaults to 100.\n",
    "            freq (int, optional): Control the frequency of logging. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "        self.tokenizer = tokenizer\n",
    "        noise_indexes = random.sample(range(0, len(val_dataset)-1), num_samples)\n",
    "        self.sample_dataset = [val_dataset[noise_index] for noise_index in noise_indexes]\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "    def on_evaluate(self, args, state, control,  **kwargs):\n",
    "        super().on_evaluate(args, state, control, **kwargs)\n",
    "        # control the frequency of logging by logging the predictions every `freq` epochs\n",
    "        if state.epoch % self.freq == 0:\n",
    "          # generate predictions\n",
    "          predictions = self.trainer.predict(self.sample_dataset)\n",
    "          # decode predictions and labels\n",
    "          predictions = decode_predictions(self.tokenizer, predictions)\n",
    "          # add predictions to a wandb.Table\n",
    "          predictions_df = pd.DataFrame(predictions)\n",
    "          predictions_df[\"epoch\"] = state.epoch\n",
    "          records_table = self._wandb.Table(dataframe=predictions_df)\n",
    "          # log the table to wandb\n",
    "          self._wandb.log({\"sample_predictions\": records_table})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cMP7tAIFDLL",
   "metadata": {
    "id": "8cMP7tAIFDLL"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"Noisy Lyrics Transcription\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "u4w-6iurbUFg",
   "metadata": {
    "id": "u4w-6iurbUFg"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"whisper-medium-finetuned-jamendo\"\n",
    "TRAIN_BATCHSIZE = 16\n",
    "LEARNING_RATE = 1e-5\n",
    "WARMUP = 5\n",
    "EVAL_BATCHSIZE = 5\n",
    "RESUME_FROM_CKPT = None\n",
    "NUM_EPOCHS = 50\n",
    "GRADIENT_CHECKPOINTING= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "th7u1JFBFQut",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "th7u1JFBFQut",
    "outputId": "f21e7252-16dc-4d71-8f53-2a83b04b6e4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir= OUTPUT_DIR,\n",
    "        per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP,\n",
    "        gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end = True,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_eval_batch_size=EVAL_BATCHSIZE,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=225,\n",
    "        logging_strategy='epoch',\n",
    "        report_to=[\"wandb\"],\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        optim=\"adamw_hf\",\n",
    "        resume_from_checkpoint=RESUME_FROM_CKPT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "-bN3ajlV5eLz",
   "metadata": {
    "id": "-bN3ajlV5eLz"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "TxjCKRfh9Rx2",
   "metadata": {
    "id": "TxjCKRfh9Rx2"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=5)\n",
    "progress_callback = WandbPredictionProgressCallback(trainer, processor.tokenizer, valid_dataset, 10,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fndcnOHFz-C0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "fndcnOHFz-C0",
    "outputId": "8f933501-ddef-4311-a430-436b11385a71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING IN PROGRESS...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240606_051721-ykwku1x0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/drigba/Noisy%20Lyrics%20Transcription/runs/ykwku1x0' target=\"_blank\">whisper-medium-finetuned-jamendo</a></strong> to <a href='https://wandb.ai/drigba/Noisy%20Lyrics%20Transcription' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/drigba/Noisy%20Lyrics%20Transcription' target=\"_blank\">https://wandb.ai/drigba/Noisy%20Lyrics%20Transcription</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/drigba/Noisy%20Lyrics%20Transcription/runs/ykwku1x0' target=\"_blank\">https://wandb.ai/drigba/Noisy%20Lyrics%20Transcription/runs/ykwku1x0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='2200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  40/2200 02:36 < 2:28:41, 0.24 it/s, Epoch 0.89/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)\n",
    "\n",
    "trainer.add_callback(progress_callback)\n",
    "trainer.add_callback(early_stopping)\n",
    "print('TRAINING IN PROGRESS...')\n",
    "trainer.train()\n",
    "print('DONE TRAINING')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae53dc",
   "metadata": {
    "id": "Wmq2G2Gc1And"
   },
   "source": [
    "## Speech enhancement module (WaveUNet)\n",
    "- WaveUNet takes 24kHz sampled input audio\n",
    "- Noise augmented audio sequence -> SE module -> denoised audio sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install denoisers --quiet\n",
    "!pip install pedalboard --quiet\n",
    "!pip install pytorch_lightning --quiet\n",
    "!pip install pesq --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173beab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from denoisers import WaveUNetModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = WaveUNetModel.from_pretrained(\"wrice/waveunet-vctk-24khz\")\n",
    "\n",
    "testdata_idx = 3 # select test data index\n",
    "audio = test_dataset[testdata_idx]['raw_audio']\n",
    "\n",
    "def denoise_audio(audio):\n",
    "    if SR != model.config.sample_rate: # SR = 16000, model.config.sample_rate=24000\n",
    "        audio = torchaudio.functional.resample(torch.FloatTensor([audio]), SR, model.config.sample_rate)\n",
    "\n",
    "    if audio.size(0) > 1:\n",
    "        audio = audio.mean(0, keepdim=True)\n",
    "\n",
    "    chunk_size = model.config.max_length\n",
    "\n",
    "    padding = abs(audio.size(-1) % chunk_size - chunk_size)\n",
    "    padded = torch.nn.functional.pad(audio, (0, padding))\n",
    "\n",
    "    clean = []\n",
    "    for i in tqdm(range(0, padded.shape[-1], chunk_size)):\n",
    "        audio_chunk = padded[:, i:i + chunk_size]\n",
    "        with torch.no_grad():\n",
    "            clean_chunk = model(audio_chunk[None]).audio\n",
    "        clean.append(clean_chunk.squeeze(0))\n",
    "\n",
    "    denoised_inference_data = torch.concat(clean, 1)[:, :audio.shape[-1]]\n",
    "    return denoised_inference_data\n",
    "\n",
    "denoised_inference_data = denoise_audio(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the original audio\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "ipd.Audio(test_dataset[testdata_idx]['raw_audio'], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the denoised audio\n",
    "ipd.Audio(np.array(denoised_inference_data[0]), rate=24000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
