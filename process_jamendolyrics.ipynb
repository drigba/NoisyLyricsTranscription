{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a85e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/f90/jamendolyrics.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc43dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "import librosa\n",
    "\n",
    "SR = 44100\n",
    "mp3_path = '/content/jamendolyrics/mp3/'\n",
    "csv_path = '/content/jamendolyrics/annotations/lines/'\n",
    "file_names = sorted(glob.glob(csv_path + '*.csv'))\n",
    "\n",
    "gather_n = 4 ################################################### Combine n data samples into one data\n",
    "len_files = []\n",
    "start_time = []\n",
    "end_time = []\n",
    "audio_types = []\n",
    "lyrics = []\n",
    "audio_dict = {} # load audio array in advance to use less RAM. key: 0~78, value: np.array\n",
    "\n",
    "for f in file_names:\n",
    "    c = pd.read_csv(f)\n",
    "    len_files.append(len(c))\n",
    "\n",
    "def preprocess_dataset():\n",
    "    for num, filename in enumerate(file_names):\n",
    "        len_csv = len_files[num]\n",
    "        with open(f\"{filename}\", \"r\") as f:\n",
    "            csv_data = csv.reader(f)\n",
    "            audio_name = filename.split('/')[-1][:-4] + '.mp3'\n",
    "            \n",
    "            print(f'{num}th csv file (filename: {audio_name})')\n",
    "            for i, line in enumerate(csv_data):\n",
    "                if i== 0: # except first head\n",
    "                    continue\n",
    "                    \n",
    "                if gather_n == 1: # not combine samples\n",
    "                    st = float(line[0])\n",
    "                    et = float(line[1])\n",
    "                    lyric = line[2]\n",
    "                    \n",
    "                    start_time.append(st)\n",
    "                    end_time.append(et)\n",
    "                    lyrics.append(lyric)\n",
    "                    audio_types.append(num)\n",
    "                    \n",
    "                else:\n",
    "                    if i%gather_n == 1: # start gathering frames\n",
    "                        st = float(line[0])\n",
    "                        et = float(line[1])\n",
    "                        lyric = line[2]\n",
    "                    else:\n",
    "                        et = float(line[1])\n",
    "                        lyric += (' ' + line[2])\n",
    "\n",
    "                    if i%gather_n == 0 or i == len_csv: # end of the group or the last line\n",
    "                        start_time.append(st)\n",
    "                        end_time.append(et)\n",
    "                        lyrics.append(lyric)\n",
    "                        audio_types.append(num)\n",
    "\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86360ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original audio arrays in advance\n",
    "for i, filename in enumerate(file_names):\n",
    "    audio_name = filename.split('/')[-1][:-4] + '.mp3'\n",
    "    audio_dict[i], _ = librosa.load(mp3_path + audio_name, sr=SR)\n",
    "    \n",
    "trimmed_audios = []\n",
    "def trim_audio(audio_name, st, et):\n",
    "    start_frame = SR*st\n",
    "    end_frame = SR*et\n",
    "    audio  = audio_dict[audio_name] # sr=44100\n",
    "    trimmed_audio = audio[int(start_frame):int(end_frame)+1]\n",
    "\n",
    "    return trimmed_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_type, st, et in zip(audio_types, start_time, end_time):\n",
    "    trimmed_audios.append(trim_audio(audio_type, st, et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, trimmed_audios, lyrics):\n",
    "        self.lyrics = lyrics\n",
    "        self.trimmed_audios = trimmed_audios\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lyrics)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'audio': self.trimmed_audios[index], 'text': self.lyrics[index]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48288eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_dataset = PairedDataset(trimmed_audios, lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d475f",
   "metadata": {},
   "source": [
    "### Approach 1. Use pytorch dataset format (I'm not sure if this format is also available in huggingface Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d72369",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Approach 1. Use pytorch dataset format (I'm not sure if this format is also available in huggingface Trainer)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "len_full_dataset = len(paired_dataset)\n",
    "\n",
    "# train samples\n",
    "train_p = 0.8\n",
    "len_train = int(len_full_dataset * train_p)\n",
    "\n",
    "# valid samples\n",
    "valid_p = 0.1\n",
    "len_valid = int(len_full_dataset * 0.1)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(paired_dataset, [len_train, len_full_dataset-len_train])\n",
    "valid_dataset, test_dataset = random_split(valid_dataset, [len_valid, len(valid_dataset)-len_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the audio aligns with text well\n",
    "import IPython.display as ipd\n",
    "idx = 20\n",
    "print(train_dataset[idx]['text'])\n",
    "ipd.Audio(train_dataset[idx]['audio'], rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49eb521",
   "metadata": {},
   "source": [
    "### Approach 2. convert to huggingface's dataset format (Requires RAM a lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Approach 2. convert to huggingface's dataset format (Requires RAM a lot)\n",
    "from datasets import Dataset\n",
    "dset = Dataset.from_list(paired_dataset)\n",
    "\n",
    "jamendo_dataset = dset.train_test_split(test_size=0.2)\n",
    "train_trainvalid_p = jamendo_dataset['test'].train_test_split(test_size=0.2)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid_p = train_trainvalid_p['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "paired_ds = DatasetDict({\n",
    "    'train': train_trainvalid_p['train'],\n",
    "    'test': test_valid_p['test'],\n",
    "    'valid': test_valid_p['train']})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
