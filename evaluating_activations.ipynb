{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5ed9e11a",
      "metadata": {
        "id": "5ed9e11a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea736e5-4a42-4f30-cc72-435f575728fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/542.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install datasets --quiet\n",
        "!pip install accelerate -U --quiet\n",
        "!pip install evaluate --quiet\n",
        "!pip install jiwer -U  -q\n",
        "!pip install transformers -U -q\n",
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b1a85e70",
      "metadata": {
        "id": "b1a85e70"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/f90/jamendolyrics.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "OhjulrrcPkKl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhjulrrcPkKl",
        "outputId": "5394de44-d498-4115-fd63-8e05ba582b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=13bUduvbSf4AowpM5NN-owNKUYdQsrfm_\n",
            "From (redirected): https://drive.google.com/uc?id=13bUduvbSf4AowpM5NN-owNKUYdQsrfm_&confirm=t&uuid=0dfffc24-9af3-4568-8d01-00864f522789\n",
            "To: /content/musan_resampled.zip\n",
            "100% 1.61G/1.61G [00:13<00:00, 115MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 13bUduvbSf4AowpM5NN-owNKUYdQsrfm_\n",
        "!unzip -q /content/musan_resampled.zip\n",
        "!rm /content/musan_resampled.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "LzuC3YKrkKcH",
      "metadata": {
        "collapsed": true,
        "id": "LzuC3YKrkKcH"
      },
      "outputs": [],
      "source": [
        "# !wget https://www.openslr.org/resources/17/musan.tar.gz\n",
        "# !tar -zxf musan.tar.gz\n",
        "# !rm -rf /content/musan/music\n",
        "# !rm -rf /content/musan/speech\n",
        "# !rm /content/musan.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "sXpWUXwgBn2E",
      "metadata": {
        "id": "sXpWUXwgBn2E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import argparse\n",
        "import evaluate\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from datasets import DatasetDict, Audio, load_from_disk, concatenate_datasets\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import wandb\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torchaudio\n",
        "import soundfile\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "import pandas as pd\n",
        "import csv\n",
        "import librosa\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "VAfD3Cn8HQuc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "VAfD3Cn8HQuc",
        "outputId": "d3ff5bde-3c46-47e4-a71d-e6825e4a2ae0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "GZhJijLABzUr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GZhJijLABzUr",
        "outputId": "cd8aeecf-59f3-46ff-d040-c30f6b4bf990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"openai/whisper-medium\"\n",
        "# LANGUAGE = \"en\"\n",
        "FREEZE_FEATURE_ENCODER = False\n",
        "FREEZE_ENCODER = False\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME,  task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME,task=\"transcribe\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "hc0w1TYVKO4X",
      "metadata": {
        "id": "hc0w1TYVKO4X"
      },
      "outputs": [],
      "source": [
        "if FREEZE_FEATURE_ENCODER:\n",
        "    model.freeze_feature_encoder()\n",
        "\n",
        "if FREEZE_ENCODER:\n",
        "    model.freeze_encoder()\n",
        "    model.model.encoder.gradient_checkpointing = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "DrAjL5bRMn26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "DrAjL5bRMn26",
        "outputId": "315ec7ec-1ca4-45bc-c573-d27da7a31767"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 URL  \\\n",
              "0  https://www.jamendo.com/track/1559261/give-me-...   \n",
              "1      https://www.jamendo.com/track/1552064/keep-on   \n",
              "2  https://www.jamendo.com/track/1537288/back-in-...   \n",
              "3       https://www.jamendo.com/track/1442030/peyote   \n",
              "4       https://www.jamendo.com/track/1465148/embers   \n",
              "\n",
              "                         Filepath            Artist             Title Genre  \\\n",
              "0     HILA_-_Give_Me_the_Same.mp3              HILA  Give Me The Same   Pop   \n",
              "1  Quentin_Hannappe_-_Keep_On.mp3  Quentin Hannappe           Keep On   Pop   \n",
              "2  Songwriterz_-_Back_In_Time.mp3       Songwriterz      Back In Time   Pop   \n",
              "3          Kinematic_-_Peyote.mp3         KINEMATIC            Peyote  Rock   \n",
              "4           Avercage_-_Embers.mp3          Avercage            Embers  Rock   \n",
              "\n",
              "  LicenseType Language  LyricOverlap  Polyphonic  NonLexical  \n",
              "0       BY-ND  English         False       False       False  \n",
              "1    BY-NC-ND  English         False       False       False  \n",
              "2       BY-ND  English         False       False        True  \n",
              "3       BY-ND  English         False        True       False  \n",
              "4    BY-NC-SA  English         False       False       False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce464cef-505a-456c-bcae-227eaa26335f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL</th>\n",
              "      <th>Filepath</th>\n",
              "      <th>Artist</th>\n",
              "      <th>Title</th>\n",
              "      <th>Genre</th>\n",
              "      <th>LicenseType</th>\n",
              "      <th>Language</th>\n",
              "      <th>LyricOverlap</th>\n",
              "      <th>Polyphonic</th>\n",
              "      <th>NonLexical</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.jamendo.com/track/1559261/give-me-...</td>\n",
              "      <td>HILA_-_Give_Me_the_Same.mp3</td>\n",
              "      <td>HILA</td>\n",
              "      <td>Give Me The Same</td>\n",
              "      <td>Pop</td>\n",
              "      <td>BY-ND</td>\n",
              "      <td>English</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.jamendo.com/track/1552064/keep-on</td>\n",
              "      <td>Quentin_Hannappe_-_Keep_On.mp3</td>\n",
              "      <td>Quentin Hannappe</td>\n",
              "      <td>Keep On</td>\n",
              "      <td>Pop</td>\n",
              "      <td>BY-NC-ND</td>\n",
              "      <td>English</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.jamendo.com/track/1537288/back-in-...</td>\n",
              "      <td>Songwriterz_-_Back_In_Time.mp3</td>\n",
              "      <td>Songwriterz</td>\n",
              "      <td>Back In Time</td>\n",
              "      <td>Pop</td>\n",
              "      <td>BY-ND</td>\n",
              "      <td>English</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.jamendo.com/track/1442030/peyote</td>\n",
              "      <td>Kinematic_-_Peyote.mp3</td>\n",
              "      <td>KINEMATIC</td>\n",
              "      <td>Peyote</td>\n",
              "      <td>Rock</td>\n",
              "      <td>BY-ND</td>\n",
              "      <td>English</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.jamendo.com/track/1465148/embers</td>\n",
              "      <td>Avercage_-_Embers.mp3</td>\n",
              "      <td>Avercage</td>\n",
              "      <td>Embers</td>\n",
              "      <td>Rock</td>\n",
              "      <td>BY-NC-SA</td>\n",
              "      <td>English</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce464cef-505a-456c-bcae-227eaa26335f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce464cef-505a-456c-bcae-227eaa26335f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce464cef-505a-456c-bcae-227eaa26335f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a73090ea-8e76-4745-908c-0a4f901348b7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a73090ea-8e76-4745-908c-0a4f901348b7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a73090ea-8e76-4745-908c-0a4f901348b7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metadata",
              "summary": "{\n  \"name\": \"metadata\",\n  \"rows\": 79,\n  \"fields\": [\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 79,\n        \"samples\": [\n          \"https://www.jamendo.com/track/1870094/confession/lyrics\",\n          \"https://www.jamendo.com/track/1559261/give-me-the-same\",\n          \"https://www.jamendo.com/track/1246750/besando-sapos/lyrics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Filepath\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 79,\n        \"samples\": [\n          \"Confession_-_Quesabe.mp3\",\n          \"HILA_-_Give_Me_the_Same.mp3\",\n          \"Besando_Sapos_-_Dream_Tabu.mp3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Artist\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"Avercage\",\n          \"WASARU\",\n          \"Pure Mids\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 79,\n        \"samples\": [\n          \"Confession\",\n          \"Give Me The Same\",\n          \"Besando Sapos\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Genre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"Folk\",\n          \"Country\",\n          \"Pop\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LicenseType\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"CC  BY-NC-SA\",\n          \"CC BY-SA\",\n          \"BY-ND\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"German\",\n          \"French\",\n          \"English\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LyricOverlap\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Polyphonic\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NonLexical\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "metadata = pd.read_csv('/content/jamendolyrics/JamendoLyrics.csv')\n",
        "metadata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "W9sXRRa4M3ho",
      "metadata": {
        "id": "W9sXRRa4M3ho"
      },
      "outputs": [],
      "source": [
        "metadata = metadata.set_index(\"Filepath\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cbc43dd3",
      "metadata": {
        "id": "cbc43dd3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "SR = 16000\n",
        "mp3_path = '/content/jamendolyrics/mp3/'\n",
        "csv_path = '/content/jamendolyrics/annotations/lines/'\n",
        "file_names = sorted(glob.glob(csv_path + '*.csv'))\n",
        "\n",
        "gather_n = 4 ################################################### Combine n data samples into one data\n",
        "len_files = []\n",
        "start_time = []\n",
        "end_time = []\n",
        "audio_types = []\n",
        "lyrics = []\n",
        "languages = []\n",
        "audio_dict = {} # load audio array in advance to use less RAM. key: 0~78, value: np.array\n",
        "\n",
        "for f in file_names:\n",
        "    c = pd.read_csv(f)\n",
        "    len_files.append(len(c))\n",
        "\n",
        "def preprocess_dataset():\n",
        "    for num, filename in enumerate(file_names):\n",
        "        len_csv = len_files[num]\n",
        "        with open(f\"{filename}\", \"r\") as f:\n",
        "            csv_data = csv.reader(f)\n",
        "            audio_name = filename.split('/')[-1][:-4] + '.mp3'\n",
        "            language = metadata.loc[audio_name][\"Language\"].lower()\n",
        "            processor.tokenizer.set_prefix_tokens(language=language)\n",
        "\n",
        "            for i, line in enumerate(csv_data):\n",
        "                if i== 0: # except first head\n",
        "                    continue\n",
        "\n",
        "                if gather_n == 1: # not combine samples\n",
        "                    st = float(line[0])\n",
        "                    et = float(line[1])\n",
        "                    lyric = line[2]\n",
        "                    lyric = processor.tokenizer(lyric).input_ids\n",
        "\n",
        "                    start_time.append(st)\n",
        "                    end_time.append(et)\n",
        "                    lyrics.append(lyric)\n",
        "                    audio_types.append(num)\n",
        "\n",
        "                else:\n",
        "                    if i%gather_n == 1: # start gathering frames\n",
        "                        st = float(line[0])\n",
        "                        et = float(line[1])\n",
        "                        lyric = line[2]\n",
        "\n",
        "                    else:\n",
        "                        et = float(line[1])\n",
        "                        lyric += (' ' + line[2])\n",
        "\n",
        "                    if i%gather_n == 0 or i == len_csv: # end of the group or the last line\n",
        "                        start_time.append(st)\n",
        "                        end_time.append(et)\n",
        "                        lyric = processor.tokenizer(lyric).input_ids\n",
        "                        lyrics.append(lyric)\n",
        "                        audio_types.append(num)\n",
        "\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "86360ee4",
      "metadata": {
        "collapsed": true,
        "id": "86360ee4"
      },
      "outputs": [],
      "source": [
        "preprocess_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f7ea96c2",
      "metadata": {
        "id": "f7ea96c2"
      },
      "outputs": [],
      "source": [
        "# Load original audio arrays in advance\n",
        "for i, filename in enumerate(file_names):\n",
        "    audio_name = filename.split('/')[-1][:-4] + '.mp3'\n",
        "    audio_dict[i], _ = librosa.load(mp3_path + audio_name, sr=SR)\n",
        "\n",
        "trimmed_audios = []\n",
        "def trim_audio(audio_name, st, et):\n",
        "    start_frame = SR*st\n",
        "    end_frame = SR*et\n",
        "    audio  = audio_dict[audio_name] # sr=44100\n",
        "    trimmed_audio = audio[int(start_frame):int(end_frame)+1]\n",
        "\n",
        "    return trimmed_audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "147f1351",
      "metadata": {
        "id": "147f1351"
      },
      "outputs": [],
      "source": [
        "for audio_type, st, et in zip(audio_types, start_time, end_time):\n",
        "    trimmed_audios.append(trim_audio(audio_type, st, et))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "lD1sCie3j02I",
      "metadata": {
        "id": "lD1sCie3j02I"
      },
      "outputs": [],
      "source": [
        "# noise_ds = load_dataset('./musan')\n",
        "noise_ds = load_from_disk('/content/content/musan_resampled')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "umULLfHr55AH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "umULLfHr55AH",
        "outputId": "c043c456-d128-4aa1-899e-8c4a97d6b0fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.03685588, -0.12578191,  0.02261727, ...,  0.18195425,\n",
              "        0.09405375,  0.03368825], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "trimmed_audios[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cabf3f19",
      "metadata": {
        "id": "cabf3f19"
      },
      "outputs": [],
      "source": [
        "# Pytorch Dataset\n",
        "# Have to add noise before feature extraction\n",
        "\n",
        "\n",
        "def add_noise_to_audio(audio, noise, clean_db, noise_snr):\n",
        "    noise_db = 10 * np.log10(np.mean(noise ** 2)+1e-4)\n",
        "    noises = [np.sqrt(10 ** ((clean_db - noise_db - noise_snr) / 10)) * noise]\n",
        "\n",
        "    if len(noises[0]) < len(audio):\n",
        "        added = np.pad(noises[0], (0, len(audio)-len(noises[0])), 'constant') + audio\n",
        "    else:\n",
        "        added = audio + noises[0][:len(audio)]\n",
        "\n",
        "    return added\n",
        "\n",
        "class PairedDataset(Dataset):\n",
        "    def __init__(self, trimmed_audios, lyrics,noise_ds,snr = 10,random_augment = True):\n",
        "        self.lyrics = lyrics\n",
        "        self.trimmed_audios = trimmed_audios\n",
        "        self.noise_dataset = noise_ds\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.snr = snr\n",
        "\n",
        "        self.random_augment = random_augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lyrics)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.random_augment:\n",
        "          noise_index = random.randint(0, len(self.noise_dataset) - 1)\n",
        "        else:\n",
        "          noise_index = index % len(self.noise_dataset)\n",
        "\n",
        "        noise_sample = self.noise_dataset[noise_index]\n",
        "        noise_array = noise_sample['audio']['array'] * 30\n",
        "        audio_array = self.trimmed_audios[index]\n",
        "\n",
        "        clean_db = 10 * np.log10(np.mean(audio_array ** 2)+1e-4) ##\n",
        "        audio_array = audio_array.squeeze()#.numpy()\n",
        "        noisy_audio = add_noise_to_audio(audio_array, noise_array, clean_db, noise_snr=self.snr)\n",
        "\n",
        "        audio_processed = self.feature_extractor(noisy_audio, sampling_rate=SR).input_features[0]\n",
        "\n",
        "        return {'input_features': audio_processed, 'labels': self.lyrics[index]}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "48288eb8",
      "metadata": {
        "id": "48288eb8"
      },
      "outputs": [],
      "source": [
        "SNR = -3\n",
        "paired_dataset = PairedDataset(trimmed_audios, lyrics, noise_ds,snr = SNR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072d475f",
      "metadata": {
        "id": "072d475f"
      },
      "source": [
        "### Approach 1. Use pytorch dataset format (I'm not sure if this format is also available in huggingface Trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "48d72369",
      "metadata": {
        "id": "48d72369"
      },
      "outputs": [],
      "source": [
        "### Approach 1. Use pytorch dataset format (I'm not sure if this format is also available in huggingface Trainer)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import random_split\n",
        "len_full_dataset = len(paired_dataset)\n",
        "\n",
        "# train samples\n",
        "train_p = 0.8\n",
        "len_train = int(len_full_dataset * train_p)\n",
        "\n",
        "# valid samples\n",
        "valid_p = 0.1\n",
        "len_valid = int(len_full_dataset * 0.1)\n",
        "\n",
        "train_dataset, valid_dataset = random_split(paired_dataset, [len_train, len_full_dataset-len_train])\n",
        "valid_dataset, test_dataset = random_split(valid_dataset, [len_valid, len(valid_dataset)-len_valid])\n",
        "\n",
        "train_dataset.random_augment = True\n",
        "valid_dataset.random_augment = False\n",
        "test_dataset.random_augment = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fda7d1ab",
      "metadata": {
        "id": "fda7d1ab"
      },
      "outputs": [],
      "source": [
        "# # Check if the audio aligns with text well\n",
        "# import IPython.display as ipd\n",
        "# idx = 20\n",
        "# print(train_dataset[idx]['labels'])\n",
        "# ipd.Audio(train_dataset[idx]['input_features'], rate=44100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "TGlE7CG5vsrT",
      "metadata": {
        "id": "TGlE7CG5vsrT"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\", padding=True)\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "uibPFbArvy23",
      "metadata": {
        "id": "uibPFbArvy23"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0EmVp_4wOQgQ",
      "metadata": {
        "id": "0EmVp_4wOQgQ"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"wer\")\n",
        "DO_NORMALIZE_EVAL = True\n",
        "normalizer = BasicTextNormalizer()\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    if DO_NORMALIZE_EVAL:\n",
        "        pred_str = [normalizer(pred) for pred in pred_str]\n",
        "        label_str = [normalizer(label) for label in label_str]\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "FKZ9X2YeW5-H",
      "metadata": {
        "id": "FKZ9X2YeW5-H"
      },
      "outputs": [],
      "source": [
        "from transformers.integrations import WandbCallback\n",
        "\n",
        "\n",
        "def decode_predictions(tokenizer, predictions):\n",
        "    pred_ids = predictions.predictions\n",
        "    label_ids = predictions.label_ids\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    return {\"labels\": label_str, \"predictions\": pred_str}\n",
        "\n",
        "\n",
        "class WandbPredictionProgressCallback(WandbCallback):\n",
        "    \"\"\"Custom WandbCallback to log model predictions during training.\n",
        "\n",
        "    This callback logs model predictions and labels to a wandb.Table at each logging step during training.\n",
        "    It allows to visualize the model predictions as the training progresses.\n",
        "\n",
        "    Attributes:\n",
        "        trainer (Trainer): The Hugging Face Trainer instance.\n",
        "        tokenizer (AutoTokenizer): The tokenizer associated with the model.\n",
        "        sample_dataset (Dataset): A subset of the validation dataset for generating predictions.\n",
        "        num_samples (int, optional): Number of samples to select from the validation dataset for generating predictions. Defaults to 100.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, trainer, tokenizer, val_dataset, num_samples=100, freq=2):\n",
        "        \"\"\"Initializes the WandbPredictionProgressCallback instance.\n",
        "\n",
        "        Args:\n",
        "            trainer (Trainer): The Hugging Face Trainer instance.\n",
        "            tokenizer (AutoTokenizer): The tokenizer associated with the model.\n",
        "            val_dataset (Dataset): The validation dataset.\n",
        "            num_samples (int, optional): Number of samples to select from the validation dataset for generating predictions. Defaults to 100.\n",
        "            freq (int, optional): Control the frequency of logging. Defaults to 2.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.trainer = trainer\n",
        "        self.tokenizer = tokenizer\n",
        "        noise_indexes = random.sample(range(0, len(val_dataset)-1), num_samples)\n",
        "        self.sample_dataset = [val_dataset[noise_index] for noise_index in noise_indexes]\n",
        "        self.freq = freq\n",
        "\n",
        "\n",
        "    def on_evaluate(self, args, state, control,  **kwargs):\n",
        "        super().on_evaluate(args, state, control, **kwargs)\n",
        "        # control the frequency of logging by logging the predictions every `freq` epochs\n",
        "        if state.epoch % self.freq == 0:\n",
        "            # generate predictions\n",
        "            predictions = self.trainer.predict(self.sample_dataset)\n",
        "            # decode predictions and labels\n",
        "            predictions = decode_predictions(self.tokenizer, predictions)\n",
        "            # add predictions to a wandb.Table\n",
        "            predictions_df = pd.DataFrame(predictions)\n",
        "            predictions_df[\"epoch\"] = state.epoch\n",
        "\n",
        "            # get audio samples from sample_dataset\n",
        "\n",
        "            # add audio samples as a new column to the DataFrame\n",
        "\n",
        "            records_table = self._wandb.Table(dataframe=predictions_df)\n",
        "            # log the table to wandb\n",
        "            self._wandb.log({\"sample_predictions\": records_table})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8cMP7tAIFDLL",
      "metadata": {
        "id": "8cMP7tAIFDLL"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_PROJECT\"] = \"Noisy Lyrics Transcription\"  # name your W&B project\n",
        "os.environ[\"ENTITY\"] = \"gct634\"  # name your W&B project\n",
        "\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
        "\n",
        "# turn off watch to log faster\n",
        "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "u4w-6iurbUFg",
      "metadata": {
        "id": "u4w-6iurbUFg"
      },
      "outputs": [],
      "source": [
        "#OUTPUT_DIR = \"whisper-medium-finetuned-jamendo-snr-3\"\n",
        "OUTPUT_DIR = \"whisper-medium-finedtuned-jamendo-snr-3-Kwangbin_testing\"\n",
        "TRAIN_BATCHSIZE = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "WARMUP = 5\n",
        "EVAL_BATCHSIZE = 5\n",
        "RESUME_FROM_CKPT = None\n",
        "NUM_EPOCHS = 50\n",
        "GRADIENT_CHECKPOINTING= True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "th7u1JFBFQut",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th7u1JFBFQut",
        "outputId": "44786194-03c9-412b-f8e5-1020f8ff5343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir= OUTPUT_DIR,\n",
        "        per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
        "        gradient_accumulation_steps=1,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP,\n",
        "        gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
        "        fp16=True,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end = True,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_eval_batch_size=EVAL_BATCHSIZE,\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=225,\n",
        "        logging_strategy='epoch',\n",
        "        report_to=[\"wandb\"],\n",
        "        metric_for_best_model=\"wer\",\n",
        "        greater_is_better=False,\n",
        "        optim=\"adamw_hf\",\n",
        "        resume_from_checkpoint=RESUME_FROM_CKPT,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "-bN3ajlV5eLz",
      "metadata": {
        "id": "-bN3ajlV5eLz"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0EOpsF-WCZA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2d9b40-7b41-4769-ad2a-cf415349bade"
      },
      "id": "0EOpsF-WCZA5",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5vkxsg4LjtD-"
      },
      "id": "5vkxsg4LjtD-",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We got model and processor\n",
        "class NoisyEvaluation(WandbCallback):\n",
        "    def __init__(self, audio_without_noises_path, audio_with_noises_path, model, processor):\n",
        "        super().__init__()\n",
        "        self.model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "        self.processor = processor\n",
        "\n",
        "        # Load and normalize audio data\n",
        "        audio_without_noise, sr = librosa.load(audio_without_noises_path, sr=16000)\n",
        "        audio_without_noise = audio_without_noise / np.max(np.abs(audio_without_noise))\n",
        "        self.audio_without_noise_input = processor(audio_without_noise, sampling_rate=sr, return_tensors=\"pt\").input_features\n",
        "\n",
        "        audio_with_noise, sr = librosa.load(audio_with_noises_path, sr=16000)\n",
        "        audio_with_noise = audio_with_noise / np.max(np.abs(audio_with_noise))\n",
        "        self.audio_with_noise_input = processor(audio_with_noise, sampling_rate=sr, return_tensors=\"pt\").input_features\n",
        "\n",
        "        # Move inputs to the same device as the model\n",
        "        self.audio_without_noise_input = self.audio_without_noise_input.to(self.model.device)\n",
        "        self.audio_with_noise_input = self.audio_with_noise_input.to(self.model.device)\n",
        "\n",
        "        # The decoder input typically starts with the BOS token for Whisper\n",
        "        bos_token_id = processor.tokenizer.bos_token_id\n",
        "        self.decoder_input_ids = torch.tensor([[bos_token_id]], device=self.model.device)\n",
        "\n",
        "        # Initial activations storage\n",
        "        self.activations_without_noise = {}\n",
        "        self.activations_with_noise = {}\n",
        "        self.attentions_without_noise = {}\n",
        "        self.attentions_with_noise = {}\n",
        "\n",
        "        # Capture initial activations\n",
        "        self.capture_initial_activations()\n",
        "\n",
        "    def get_activation(self, name, store_dict):\n",
        "        def hook(model, input, output):\n",
        "            store_dict[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    def get_attention_activation(self, name, store_dict):\n",
        "        def hook(model, input, output):\n",
        "            if 'layer_norm' in name.lower() or 'norm' in name.lower():\n",
        "                store_dict[name] = output.detach()\n",
        "            else:\n",
        "                if not isinstance(output, tuple):\n",
        "                    output = (output,)\n",
        "                if len(output) > 0 and output[0] is not None:\n",
        "                    store_dict[name] = output[0].detach()\n",
        "                else:\n",
        "                    raise IndexError(f\"Expected first element in output tuple for {name}, but got {output}.\")\n",
        "        return hook\n",
        "\n",
        "    def run_model_and_capture_activations(self, input_features, activations_store, attentions_store):\n",
        "        hooks = []\n",
        "        for name, layer in self.model.named_modules():\n",
        "            if isinstance(layer, nn.Conv1d):\n",
        "                hooks.append(layer.register_forward_hook(self.get_activation(name, activations_store)))\n",
        "            elif 'self_attn' in name.lower():  # Assumption: attention layers contain 'attention' in their name\n",
        "                hooks.append(layer.register_forward_hook(self.get_attention_activation(name, attentions_store)))\n",
        "\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = self.model(input_features=input_features, decoder_input_ids=self.decoder_input_ids)\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "        return output\n",
        "\n",
        "    def mean_squared_difference(self, matrix1, matrix2):\n",
        "        difference = matrix1 - matrix2\n",
        "        squared_difference = difference ** 2\n",
        "        mean_squared_diff = torch.mean(squared_difference)\n",
        "        return mean_squared_diff\n",
        "\n",
        "    def calculate_entropy(self, tensor):\n",
        "        # Ensure the tensor is not empty\n",
        "        if tensor.numel() == 0:\n",
        "            return float('nan')\n",
        "\n",
        "        # Normalize tensor to avoid very large/small values\n",
        "        tensor = tensor / tensor.max(dim=1, keepdim=True)[0]\n",
        "\n",
        "        # Flatten the tensor to apply log_softmax along the right dimension\n",
        "        tensor = tensor.flatten(start_dim=1)\n",
        "\n",
        "        # Apply log_softmax\n",
        "        log_softmax_tensor = F.log_softmax(tensor, dim=1)\n",
        "\n",
        "        # Calculate entropy\n",
        "        exp_log_softmax_tensor = torch.exp(log_softmax_tensor)\n",
        "        entropy = -torch.sum(exp_log_softmax_tensor * log_softmax_tensor, dim=1)\n",
        "\n",
        "        # Average the entropy\n",
        "        average_entropy = torch.mean(entropy)\n",
        "\n",
        "        # Check for NaNs in the entropy calculation\n",
        "        if torch.isnan(average_entropy):\n",
        "            return float('nan')\n",
        "\n",
        "        return average_entropy.item()\n",
        "    def capture_initial_activations(self):\n",
        "        torch.cuda.empty_cache()  # Free up memory before capturing new activations\n",
        "        original_output_without_noise = self.run_model_and_capture_activations(self.audio_without_noise_input, self.activations_without_noise, self.attentions_without_noise)\n",
        "        original_output_with_noise = self.run_model_and_capture_activations(self.audio_with_noise_input, self.activations_with_noise, self.attentions_with_noise)\n",
        "\n",
        "        # Debug: print out captured keys for attention layers\n",
        "        print(\"Captured attention keys without noise:\", list(self.attentions_without_noise.keys()))\n",
        "        print(\"Captured attention keys with noise:\", list(self.attentions_with_noise.keys()))\n",
        "\n",
        "\n",
        "\n",
        "        self.original_conv1_entropy_with_noise = self.calculate_entropy(self.activations_with_noise['model.encoder.conv1'])\n",
        "        self.original_conv1_entropy_without_noise = self.calculate_entropy(self.activations_without_noise['model.encoder.conv1'])\n",
        "        self.original_conv2_entropy_with_noise = self.calculate_entropy(self.activations_with_noise['model.encoder.conv2'])\n",
        "        self.original_conv2_entropy_without_noise = self.calculate_entropy(self.activations_without_noise['model.encoder.conv2'])\n",
        "        self.original_MSD_conv1 = self.mean_squared_difference(self.activations_with_noise['model.encoder.conv1'], self.activations_without_noise['model.encoder.conv1'])\n",
        "        self.original_MSD_conv2 = self.mean_squared_difference(self.activations_with_noise['model.encoder.conv2'], self.activations_without_noise['model.encoder.conv2'])\n",
        "        self.original_MSD_attention0 = self.mean_squared_difference(self.attentions_with_noise['model.encoder.layers.0.self_attn'], self.attentions_without_noise['model.encoder.layers.0.self_attn'])\n",
        "        self.original_MSD_attention23 = self.mean_squared_difference(self.attentions_with_noise['model.encoder.layers.23.self_attn'], self.attentions_without_noise['model.encoder.layers.23.self_attn'])\n",
        "\n",
        "        print(f\"Baseline Model Convolutional Entropy (With Noise): {self.original_conv1_entropy_with_noise}\")\n",
        "        print(f\"Baseline Model Convolutional Entropy (Without Noise): {self.original_conv1_entropy_without_noise}\")\n",
        "        print(f\"Baseline Model Convolutional Entropy 2 (With Noise): {self.original_conv2_entropy_with_noise}\")\n",
        "        print(f\"Baseline Model Convolutional Entropy 2 (Without Noise): {self.original_conv2_entropy_without_noise}\")\n",
        "        print(\"Baseline Mean Squared Difference in Conv1: \", self.original_MSD_conv1)\n",
        "        print(\"Baseline Mean Squared Difference in Conv2: \", self.original_MSD_conv2)\n",
        "        print(\"Baseline Mean Squared Difference in Attention0: \", self.original_MSD_attention0)\n",
        "        print(\"Baseline Mean Squared Difference in Attention23: \", self.original_MSD_attention23)\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        super().on_epoch_end(args, state, control, **kwargs)\n",
        "        activations_without_noise = {}\n",
        "        activations_with_noise = {}\n",
        "        attentions_without_noise = {}\n",
        "        attentions_with_noise = {}\n",
        "\n",
        "        torch.cuda.empty_cache()  # Free up memory before capturing new activations\n",
        "\n",
        "        # Run the model without noise and capture activations\n",
        "        output_without_noise = self.run_model_and_capture_activations(self.audio_without_noise_input, activations_without_noise,  attentions_without_noise)\n",
        "\n",
        "        # Run the model with noise and capture activations\n",
        "        output_with_noise = self.run_model_and_capture_activations(self.audio_with_noise_input, activations_with_noise, attentions_with_noise)\n",
        "\n",
        "        print(f\"Baseline Convolutional Entropy (With Noise): {self.original_conv1_entropy_with_noise}\")\n",
        "        print(f\"Baseline Convolutional Entropy (Without Noise): {self.original_conv1_entropy_without_noise}\")\n",
        "        print(f\"Baseline Convolutional Entropy 2 (With Noise): {self.original_conv2_entropy_with_noise}\")\n",
        "        print(f\"Baseline Convolutional Entropy 2 (Without Noise): {self.original_conv2_entropy_without_noise}\")\n",
        "        print(\"Baseline Mean Squared Difference in Conv1: \", self.original_MSD_conv1)\n",
        "        print(\"Baseline Mean Squared Difference in Conv2: \", self.original_MSD_conv2)\n",
        "\n",
        "        conv1_entropy_with_noise = self.calculate_entropy(activations_with_noise['model.encoder.conv1'])\n",
        "        print(f\"Fine Tuned Convolutional Entropy (With Noise): {conv1_entropy_with_noise}\")\n",
        "\n",
        "        conv1_entropy_without_noise = self.calculate_entropy(activations_without_noise['model.encoder.conv1'])\n",
        "        print(f\"Fine Tuned Convolutional Entropy (Without Noise): {conv1_entropy_without_noise}\")\n",
        "\n",
        "        conv2_entropy_with_noise = self.calculate_entropy(activations_with_noise['model.encoder.conv2'])\n",
        "        print(f\"Fine Tuned Convolutional Entropy 2 (With Noise): {conv2_entropy_with_noise}\")\n",
        "\n",
        "        conv2_entropy_without_noise = self.calculate_entropy(activations_without_noise['model.encoder.conv2'])\n",
        "        print(f\"Fine Tuned Convolutional Entropy 2 (Without Noise): {conv2_entropy_without_noise}\")\n",
        "\n",
        "        MSD_conv1_finetuned = self.mean_squared_difference(activations_with_noise['model.encoder.conv1'], activations_without_noise['model.encoder.conv1'])\n",
        "        MSD_conv2_finetuned = self.mean_squared_difference(activations_with_noise['model.encoder.conv2'], activations_without_noise['model.encoder.conv2'])\n",
        "\n",
        "        print(\"Fine-Tuned Mean Squared Difference in Conv1: \", MSD_conv1_finetuned)\n",
        "        print(\"Fine-Tuned Mean Squared Difference in Conv2: \", MSD_conv2_finetuned)\n",
        "\n",
        "\n",
        "        encoder_attention_zero = self.mean_squared_difference(attentions_with_noise['model.encoder.layers.0.self_attn'], attentions_without_noise['model.encoder.layers.0.self_attn'])\n",
        "        encoder_attention_last = self.mean_squared_difference(attentions_with_noise['model.encoder.layers.23.self_attn'], attentions_without_noise['model.encoder.layers.23.self_attn'])\n",
        "\n",
        "\n",
        "\n",
        "        wandb.log({\n",
        "            'entropy_conv1_with_noise': conv1_entropy_with_noise,\n",
        "            'entropy_conv1_without_noise': conv1_entropy_without_noise,\n",
        "            'entropy_conv2_with_noise': conv2_entropy_with_noise,\n",
        "            'entropy_conv2_without_noise': conv2_entropy_without_noise,\n",
        "            'MSD_conv1': MSD_conv1_finetuned,\n",
        "            'MSD_conv2': MSD_conv2_finetuned,\n",
        "            'MSD Attention Layer0 ': encoder_attention_zero,\n",
        "            'MSD Attention Layer23 ': encoder_attention_last,\n",
        "            'epoch': state.epoch\n",
        "        })\n",
        "\n",
        "noisy_data_path =  \"/content/drive/MyDrive/musicml/gct634_final_project/audio_sample.wav\"\n",
        "non_noisy_data_path = \"/content/drive/MyDrive/musicml/gct634_final_project/ariana_cafe.wav\"\n",
        "\n",
        "noisy_eval = NoisyEvaluation( noisy_data_path, non_noisy_data_path, model, processor)"
      ],
      "metadata": {
        "id": "afKmnQVxt3h6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82e1d69-9d7c-4c0d-824d-e1761fd56e6b"
      },
      "id": "afKmnQVxt3h6",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captured attention keys without noise: ['model.encoder.layers.0.self_attn_layer_norm', 'model.encoder.layers.0.self_attn.q_proj', 'model.encoder.layers.0.self_attn.k_proj', 'model.encoder.layers.0.self_attn.v_proj', 'model.encoder.layers.0.self_attn.out_proj', 'model.encoder.layers.0.self_attn', 'model.encoder.layers.1.self_attn_layer_norm', 'model.encoder.layers.1.self_attn.q_proj', 'model.encoder.layers.1.self_attn.k_proj', 'model.encoder.layers.1.self_attn.v_proj', 'model.encoder.layers.1.self_attn.out_proj', 'model.encoder.layers.1.self_attn', 'model.encoder.layers.2.self_attn_layer_norm', 'model.encoder.layers.2.self_attn.q_proj', 'model.encoder.layers.2.self_attn.k_proj', 'model.encoder.layers.2.self_attn.v_proj', 'model.encoder.layers.2.self_attn.out_proj', 'model.encoder.layers.2.self_attn', 'model.encoder.layers.3.self_attn_layer_norm', 'model.encoder.layers.3.self_attn.q_proj', 'model.encoder.layers.3.self_attn.k_proj', 'model.encoder.layers.3.self_attn.v_proj', 'model.encoder.layers.3.self_attn.out_proj', 'model.encoder.layers.3.self_attn', 'model.encoder.layers.4.self_attn_layer_norm', 'model.encoder.layers.4.self_attn.q_proj', 'model.encoder.layers.4.self_attn.k_proj', 'model.encoder.layers.4.self_attn.v_proj', 'model.encoder.layers.4.self_attn.out_proj', 'model.encoder.layers.4.self_attn', 'model.encoder.layers.5.self_attn_layer_norm', 'model.encoder.layers.5.self_attn.q_proj', 'model.encoder.layers.5.self_attn.k_proj', 'model.encoder.layers.5.self_attn.v_proj', 'model.encoder.layers.5.self_attn.out_proj', 'model.encoder.layers.5.self_attn', 'model.encoder.layers.6.self_attn_layer_norm', 'model.encoder.layers.6.self_attn.q_proj', 'model.encoder.layers.6.self_attn.k_proj', 'model.encoder.layers.6.self_attn.v_proj', 'model.encoder.layers.6.self_attn.out_proj', 'model.encoder.layers.6.self_attn', 'model.encoder.layers.7.self_attn_layer_norm', 'model.encoder.layers.7.self_attn.q_proj', 'model.encoder.layers.7.self_attn.k_proj', 'model.encoder.layers.7.self_attn.v_proj', 'model.encoder.layers.7.self_attn.out_proj', 'model.encoder.layers.7.self_attn', 'model.encoder.layers.8.self_attn_layer_norm', 'model.encoder.layers.8.self_attn.q_proj', 'model.encoder.layers.8.self_attn.k_proj', 'model.encoder.layers.8.self_attn.v_proj', 'model.encoder.layers.8.self_attn.out_proj', 'model.encoder.layers.8.self_attn', 'model.encoder.layers.9.self_attn_layer_norm', 'model.encoder.layers.9.self_attn.q_proj', 'model.encoder.layers.9.self_attn.k_proj', 'model.encoder.layers.9.self_attn.v_proj', 'model.encoder.layers.9.self_attn.out_proj', 'model.encoder.layers.9.self_attn', 'model.encoder.layers.10.self_attn_layer_norm', 'model.encoder.layers.10.self_attn.q_proj', 'model.encoder.layers.10.self_attn.k_proj', 'model.encoder.layers.10.self_attn.v_proj', 'model.encoder.layers.10.self_attn.out_proj', 'model.encoder.layers.10.self_attn', 'model.encoder.layers.11.self_attn_layer_norm', 'model.encoder.layers.11.self_attn.q_proj', 'model.encoder.layers.11.self_attn.k_proj', 'model.encoder.layers.11.self_attn.v_proj', 'model.encoder.layers.11.self_attn.out_proj', 'model.encoder.layers.11.self_attn', 'model.encoder.layers.12.self_attn_layer_norm', 'model.encoder.layers.12.self_attn.q_proj', 'model.encoder.layers.12.self_attn.k_proj', 'model.encoder.layers.12.self_attn.v_proj', 'model.encoder.layers.12.self_attn.out_proj', 'model.encoder.layers.12.self_attn', 'model.encoder.layers.13.self_attn_layer_norm', 'model.encoder.layers.13.self_attn.q_proj', 'model.encoder.layers.13.self_attn.k_proj', 'model.encoder.layers.13.self_attn.v_proj', 'model.encoder.layers.13.self_attn.out_proj', 'model.encoder.layers.13.self_attn', 'model.encoder.layers.14.self_attn_layer_norm', 'model.encoder.layers.14.self_attn.q_proj', 'model.encoder.layers.14.self_attn.k_proj', 'model.encoder.layers.14.self_attn.v_proj', 'model.encoder.layers.14.self_attn.out_proj', 'model.encoder.layers.14.self_attn', 'model.encoder.layers.15.self_attn_layer_norm', 'model.encoder.layers.15.self_attn.q_proj', 'model.encoder.layers.15.self_attn.k_proj', 'model.encoder.layers.15.self_attn.v_proj', 'model.encoder.layers.15.self_attn.out_proj', 'model.encoder.layers.15.self_attn', 'model.encoder.layers.16.self_attn_layer_norm', 'model.encoder.layers.16.self_attn.q_proj', 'model.encoder.layers.16.self_attn.k_proj', 'model.encoder.layers.16.self_attn.v_proj', 'model.encoder.layers.16.self_attn.out_proj', 'model.encoder.layers.16.self_attn', 'model.encoder.layers.17.self_attn_layer_norm', 'model.encoder.layers.17.self_attn.q_proj', 'model.encoder.layers.17.self_attn.k_proj', 'model.encoder.layers.17.self_attn.v_proj', 'model.encoder.layers.17.self_attn.out_proj', 'model.encoder.layers.17.self_attn', 'model.encoder.layers.18.self_attn_layer_norm', 'model.encoder.layers.18.self_attn.q_proj', 'model.encoder.layers.18.self_attn.k_proj', 'model.encoder.layers.18.self_attn.v_proj', 'model.encoder.layers.18.self_attn.out_proj', 'model.encoder.layers.18.self_attn', 'model.encoder.layers.19.self_attn_layer_norm', 'model.encoder.layers.19.self_attn.q_proj', 'model.encoder.layers.19.self_attn.k_proj', 'model.encoder.layers.19.self_attn.v_proj', 'model.encoder.layers.19.self_attn.out_proj', 'model.encoder.layers.19.self_attn', 'model.encoder.layers.20.self_attn_layer_norm', 'model.encoder.layers.20.self_attn.q_proj', 'model.encoder.layers.20.self_attn.k_proj', 'model.encoder.layers.20.self_attn.v_proj', 'model.encoder.layers.20.self_attn.out_proj', 'model.encoder.layers.20.self_attn', 'model.encoder.layers.21.self_attn_layer_norm', 'model.encoder.layers.21.self_attn.q_proj', 'model.encoder.layers.21.self_attn.k_proj', 'model.encoder.layers.21.self_attn.v_proj', 'model.encoder.layers.21.self_attn.out_proj', 'model.encoder.layers.21.self_attn', 'model.encoder.layers.22.self_attn_layer_norm', 'model.encoder.layers.22.self_attn.q_proj', 'model.encoder.layers.22.self_attn.k_proj', 'model.encoder.layers.22.self_attn.v_proj', 'model.encoder.layers.22.self_attn.out_proj', 'model.encoder.layers.22.self_attn', 'model.encoder.layers.23.self_attn_layer_norm', 'model.encoder.layers.23.self_attn.q_proj', 'model.encoder.layers.23.self_attn.k_proj', 'model.encoder.layers.23.self_attn.v_proj', 'model.encoder.layers.23.self_attn.out_proj', 'model.encoder.layers.23.self_attn', 'model.decoder.layers.0.self_attn_layer_norm', 'model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.0.self_attn.out_proj', 'model.decoder.layers.0.self_attn', 'model.decoder.layers.1.self_attn_layer_norm', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.1.self_attn.out_proj', 'model.decoder.layers.1.self_attn', 'model.decoder.layers.2.self_attn_layer_norm', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.2.self_attn.out_proj', 'model.decoder.layers.2.self_attn', 'model.decoder.layers.3.self_attn_layer_norm', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.3.self_attn.out_proj', 'model.decoder.layers.3.self_attn', 'model.decoder.layers.4.self_attn_layer_norm', 'model.decoder.layers.4.self_attn.q_proj', 'model.decoder.layers.4.self_attn.k_proj', 'model.decoder.layers.4.self_attn.v_proj', 'model.decoder.layers.4.self_attn.out_proj', 'model.decoder.layers.4.self_attn', 'model.decoder.layers.5.self_attn_layer_norm', 'model.decoder.layers.5.self_attn.q_proj', 'model.decoder.layers.5.self_attn.k_proj', 'model.decoder.layers.5.self_attn.v_proj', 'model.decoder.layers.5.self_attn.out_proj', 'model.decoder.layers.5.self_attn', 'model.decoder.layers.6.self_attn_layer_norm', 'model.decoder.layers.6.self_attn.q_proj', 'model.decoder.layers.6.self_attn.k_proj', 'model.decoder.layers.6.self_attn.v_proj', 'model.decoder.layers.6.self_attn.out_proj', 'model.decoder.layers.6.self_attn', 'model.decoder.layers.7.self_attn_layer_norm', 'model.decoder.layers.7.self_attn.q_proj', 'model.decoder.layers.7.self_attn.k_proj', 'model.decoder.layers.7.self_attn.v_proj', 'model.decoder.layers.7.self_attn.out_proj', 'model.decoder.layers.7.self_attn', 'model.decoder.layers.8.self_attn_layer_norm', 'model.decoder.layers.8.self_attn.q_proj', 'model.decoder.layers.8.self_attn.k_proj', 'model.decoder.layers.8.self_attn.v_proj', 'model.decoder.layers.8.self_attn.out_proj', 'model.decoder.layers.8.self_attn', 'model.decoder.layers.9.self_attn_layer_norm', 'model.decoder.layers.9.self_attn.q_proj', 'model.decoder.layers.9.self_attn.k_proj', 'model.decoder.layers.9.self_attn.v_proj', 'model.decoder.layers.9.self_attn.out_proj', 'model.decoder.layers.9.self_attn', 'model.decoder.layers.10.self_attn_layer_norm', 'model.decoder.layers.10.self_attn.q_proj', 'model.decoder.layers.10.self_attn.k_proj', 'model.decoder.layers.10.self_attn.v_proj', 'model.decoder.layers.10.self_attn.out_proj', 'model.decoder.layers.10.self_attn', 'model.decoder.layers.11.self_attn_layer_norm', 'model.decoder.layers.11.self_attn.q_proj', 'model.decoder.layers.11.self_attn.k_proj', 'model.decoder.layers.11.self_attn.v_proj', 'model.decoder.layers.11.self_attn.out_proj', 'model.decoder.layers.11.self_attn', 'model.decoder.layers.12.self_attn_layer_norm', 'model.decoder.layers.12.self_attn.q_proj', 'model.decoder.layers.12.self_attn.k_proj', 'model.decoder.layers.12.self_attn.v_proj', 'model.decoder.layers.12.self_attn.out_proj', 'model.decoder.layers.12.self_attn', 'model.decoder.layers.13.self_attn_layer_norm', 'model.decoder.layers.13.self_attn.q_proj', 'model.decoder.layers.13.self_attn.k_proj', 'model.decoder.layers.13.self_attn.v_proj', 'model.decoder.layers.13.self_attn.out_proj', 'model.decoder.layers.13.self_attn', 'model.decoder.layers.14.self_attn_layer_norm', 'model.decoder.layers.14.self_attn.q_proj', 'model.decoder.layers.14.self_attn.k_proj', 'model.decoder.layers.14.self_attn.v_proj', 'model.decoder.layers.14.self_attn.out_proj', 'model.decoder.layers.14.self_attn', 'model.decoder.layers.15.self_attn_layer_norm', 'model.decoder.layers.15.self_attn.q_proj', 'model.decoder.layers.15.self_attn.k_proj', 'model.decoder.layers.15.self_attn.v_proj', 'model.decoder.layers.15.self_attn.out_proj', 'model.decoder.layers.15.self_attn', 'model.decoder.layers.16.self_attn_layer_norm', 'model.decoder.layers.16.self_attn.q_proj', 'model.decoder.layers.16.self_attn.k_proj', 'model.decoder.layers.16.self_attn.v_proj', 'model.decoder.layers.16.self_attn.out_proj', 'model.decoder.layers.16.self_attn', 'model.decoder.layers.17.self_attn_layer_norm', 'model.decoder.layers.17.self_attn.q_proj', 'model.decoder.layers.17.self_attn.k_proj', 'model.decoder.layers.17.self_attn.v_proj', 'model.decoder.layers.17.self_attn.out_proj', 'model.decoder.layers.17.self_attn', 'model.decoder.layers.18.self_attn_layer_norm', 'model.decoder.layers.18.self_attn.q_proj', 'model.decoder.layers.18.self_attn.k_proj', 'model.decoder.layers.18.self_attn.v_proj', 'model.decoder.layers.18.self_attn.out_proj', 'model.decoder.layers.18.self_attn', 'model.decoder.layers.19.self_attn_layer_norm', 'model.decoder.layers.19.self_attn.q_proj', 'model.decoder.layers.19.self_attn.k_proj', 'model.decoder.layers.19.self_attn.v_proj', 'model.decoder.layers.19.self_attn.out_proj', 'model.decoder.layers.19.self_attn', 'model.decoder.layers.20.self_attn_layer_norm', 'model.decoder.layers.20.self_attn.q_proj', 'model.decoder.layers.20.self_attn.k_proj', 'model.decoder.layers.20.self_attn.v_proj', 'model.decoder.layers.20.self_attn.out_proj', 'model.decoder.layers.20.self_attn', 'model.decoder.layers.21.self_attn_layer_norm', 'model.decoder.layers.21.self_attn.q_proj', 'model.decoder.layers.21.self_attn.k_proj', 'model.decoder.layers.21.self_attn.v_proj', 'model.decoder.layers.21.self_attn.out_proj', 'model.decoder.layers.21.self_attn', 'model.decoder.layers.22.self_attn_layer_norm', 'model.decoder.layers.22.self_attn.q_proj', 'model.decoder.layers.22.self_attn.k_proj', 'model.decoder.layers.22.self_attn.v_proj', 'model.decoder.layers.22.self_attn.out_proj', 'model.decoder.layers.22.self_attn', 'model.decoder.layers.23.self_attn_layer_norm', 'model.decoder.layers.23.self_attn.q_proj', 'model.decoder.layers.23.self_attn.k_proj', 'model.decoder.layers.23.self_attn.v_proj', 'model.decoder.layers.23.self_attn.out_proj', 'model.decoder.layers.23.self_attn']\n",
            "Captured attention keys with noise: ['model.encoder.layers.0.self_attn_layer_norm', 'model.encoder.layers.0.self_attn.q_proj', 'model.encoder.layers.0.self_attn.k_proj', 'model.encoder.layers.0.self_attn.v_proj', 'model.encoder.layers.0.self_attn.out_proj', 'model.encoder.layers.0.self_attn', 'model.encoder.layers.1.self_attn_layer_norm', 'model.encoder.layers.1.self_attn.q_proj', 'model.encoder.layers.1.self_attn.k_proj', 'model.encoder.layers.1.self_attn.v_proj', 'model.encoder.layers.1.self_attn.out_proj', 'model.encoder.layers.1.self_attn', 'model.encoder.layers.2.self_attn_layer_norm', 'model.encoder.layers.2.self_attn.q_proj', 'model.encoder.layers.2.self_attn.k_proj', 'model.encoder.layers.2.self_attn.v_proj', 'model.encoder.layers.2.self_attn.out_proj', 'model.encoder.layers.2.self_attn', 'model.encoder.layers.3.self_attn_layer_norm', 'model.encoder.layers.3.self_attn.q_proj', 'model.encoder.layers.3.self_attn.k_proj', 'model.encoder.layers.3.self_attn.v_proj', 'model.encoder.layers.3.self_attn.out_proj', 'model.encoder.layers.3.self_attn', 'model.encoder.layers.4.self_attn_layer_norm', 'model.encoder.layers.4.self_attn.q_proj', 'model.encoder.layers.4.self_attn.k_proj', 'model.encoder.layers.4.self_attn.v_proj', 'model.encoder.layers.4.self_attn.out_proj', 'model.encoder.layers.4.self_attn', 'model.encoder.layers.5.self_attn_layer_norm', 'model.encoder.layers.5.self_attn.q_proj', 'model.encoder.layers.5.self_attn.k_proj', 'model.encoder.layers.5.self_attn.v_proj', 'model.encoder.layers.5.self_attn.out_proj', 'model.encoder.layers.5.self_attn', 'model.encoder.layers.6.self_attn_layer_norm', 'model.encoder.layers.6.self_attn.q_proj', 'model.encoder.layers.6.self_attn.k_proj', 'model.encoder.layers.6.self_attn.v_proj', 'model.encoder.layers.6.self_attn.out_proj', 'model.encoder.layers.6.self_attn', 'model.encoder.layers.7.self_attn_layer_norm', 'model.encoder.layers.7.self_attn.q_proj', 'model.encoder.layers.7.self_attn.k_proj', 'model.encoder.layers.7.self_attn.v_proj', 'model.encoder.layers.7.self_attn.out_proj', 'model.encoder.layers.7.self_attn', 'model.encoder.layers.8.self_attn_layer_norm', 'model.encoder.layers.8.self_attn.q_proj', 'model.encoder.layers.8.self_attn.k_proj', 'model.encoder.layers.8.self_attn.v_proj', 'model.encoder.layers.8.self_attn.out_proj', 'model.encoder.layers.8.self_attn', 'model.encoder.layers.9.self_attn_layer_norm', 'model.encoder.layers.9.self_attn.q_proj', 'model.encoder.layers.9.self_attn.k_proj', 'model.encoder.layers.9.self_attn.v_proj', 'model.encoder.layers.9.self_attn.out_proj', 'model.encoder.layers.9.self_attn', 'model.encoder.layers.10.self_attn_layer_norm', 'model.encoder.layers.10.self_attn.q_proj', 'model.encoder.layers.10.self_attn.k_proj', 'model.encoder.layers.10.self_attn.v_proj', 'model.encoder.layers.10.self_attn.out_proj', 'model.encoder.layers.10.self_attn', 'model.encoder.layers.11.self_attn_layer_norm', 'model.encoder.layers.11.self_attn.q_proj', 'model.encoder.layers.11.self_attn.k_proj', 'model.encoder.layers.11.self_attn.v_proj', 'model.encoder.layers.11.self_attn.out_proj', 'model.encoder.layers.11.self_attn', 'model.encoder.layers.12.self_attn_layer_norm', 'model.encoder.layers.12.self_attn.q_proj', 'model.encoder.layers.12.self_attn.k_proj', 'model.encoder.layers.12.self_attn.v_proj', 'model.encoder.layers.12.self_attn.out_proj', 'model.encoder.layers.12.self_attn', 'model.encoder.layers.13.self_attn_layer_norm', 'model.encoder.layers.13.self_attn.q_proj', 'model.encoder.layers.13.self_attn.k_proj', 'model.encoder.layers.13.self_attn.v_proj', 'model.encoder.layers.13.self_attn.out_proj', 'model.encoder.layers.13.self_attn', 'model.encoder.layers.14.self_attn_layer_norm', 'model.encoder.layers.14.self_attn.q_proj', 'model.encoder.layers.14.self_attn.k_proj', 'model.encoder.layers.14.self_attn.v_proj', 'model.encoder.layers.14.self_attn.out_proj', 'model.encoder.layers.14.self_attn', 'model.encoder.layers.15.self_attn_layer_norm', 'model.encoder.layers.15.self_attn.q_proj', 'model.encoder.layers.15.self_attn.k_proj', 'model.encoder.layers.15.self_attn.v_proj', 'model.encoder.layers.15.self_attn.out_proj', 'model.encoder.layers.15.self_attn', 'model.encoder.layers.16.self_attn_layer_norm', 'model.encoder.layers.16.self_attn.q_proj', 'model.encoder.layers.16.self_attn.k_proj', 'model.encoder.layers.16.self_attn.v_proj', 'model.encoder.layers.16.self_attn.out_proj', 'model.encoder.layers.16.self_attn', 'model.encoder.layers.17.self_attn_layer_norm', 'model.encoder.layers.17.self_attn.q_proj', 'model.encoder.layers.17.self_attn.k_proj', 'model.encoder.layers.17.self_attn.v_proj', 'model.encoder.layers.17.self_attn.out_proj', 'model.encoder.layers.17.self_attn', 'model.encoder.layers.18.self_attn_layer_norm', 'model.encoder.layers.18.self_attn.q_proj', 'model.encoder.layers.18.self_attn.k_proj', 'model.encoder.layers.18.self_attn.v_proj', 'model.encoder.layers.18.self_attn.out_proj', 'model.encoder.layers.18.self_attn', 'model.encoder.layers.19.self_attn_layer_norm', 'model.encoder.layers.19.self_attn.q_proj', 'model.encoder.layers.19.self_attn.k_proj', 'model.encoder.layers.19.self_attn.v_proj', 'model.encoder.layers.19.self_attn.out_proj', 'model.encoder.layers.19.self_attn', 'model.encoder.layers.20.self_attn_layer_norm', 'model.encoder.layers.20.self_attn.q_proj', 'model.encoder.layers.20.self_attn.k_proj', 'model.encoder.layers.20.self_attn.v_proj', 'model.encoder.layers.20.self_attn.out_proj', 'model.encoder.layers.20.self_attn', 'model.encoder.layers.21.self_attn_layer_norm', 'model.encoder.layers.21.self_attn.q_proj', 'model.encoder.layers.21.self_attn.k_proj', 'model.encoder.layers.21.self_attn.v_proj', 'model.encoder.layers.21.self_attn.out_proj', 'model.encoder.layers.21.self_attn', 'model.encoder.layers.22.self_attn_layer_norm', 'model.encoder.layers.22.self_attn.q_proj', 'model.encoder.layers.22.self_attn.k_proj', 'model.encoder.layers.22.self_attn.v_proj', 'model.encoder.layers.22.self_attn.out_proj', 'model.encoder.layers.22.self_attn', 'model.encoder.layers.23.self_attn_layer_norm', 'model.encoder.layers.23.self_attn.q_proj', 'model.encoder.layers.23.self_attn.k_proj', 'model.encoder.layers.23.self_attn.v_proj', 'model.encoder.layers.23.self_attn.out_proj', 'model.encoder.layers.23.self_attn', 'model.decoder.layers.0.self_attn_layer_norm', 'model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.0.self_attn.out_proj', 'model.decoder.layers.0.self_attn', 'model.decoder.layers.1.self_attn_layer_norm', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.1.self_attn.out_proj', 'model.decoder.layers.1.self_attn', 'model.decoder.layers.2.self_attn_layer_norm', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.2.self_attn.out_proj', 'model.decoder.layers.2.self_attn', 'model.decoder.layers.3.self_attn_layer_norm', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.3.self_attn.out_proj', 'model.decoder.layers.3.self_attn', 'model.decoder.layers.4.self_attn_layer_norm', 'model.decoder.layers.4.self_attn.q_proj', 'model.decoder.layers.4.self_attn.k_proj', 'model.decoder.layers.4.self_attn.v_proj', 'model.decoder.layers.4.self_attn.out_proj', 'model.decoder.layers.4.self_attn', 'model.decoder.layers.5.self_attn_layer_norm', 'model.decoder.layers.5.self_attn.q_proj', 'model.decoder.layers.5.self_attn.k_proj', 'model.decoder.layers.5.self_attn.v_proj', 'model.decoder.layers.5.self_attn.out_proj', 'model.decoder.layers.5.self_attn', 'model.decoder.layers.6.self_attn_layer_norm', 'model.decoder.layers.6.self_attn.q_proj', 'model.decoder.layers.6.self_attn.k_proj', 'model.decoder.layers.6.self_attn.v_proj', 'model.decoder.layers.6.self_attn.out_proj', 'model.decoder.layers.6.self_attn', 'model.decoder.layers.7.self_attn_layer_norm', 'model.decoder.layers.7.self_attn.q_proj', 'model.decoder.layers.7.self_attn.k_proj', 'model.decoder.layers.7.self_attn.v_proj', 'model.decoder.layers.7.self_attn.out_proj', 'model.decoder.layers.7.self_attn', 'model.decoder.layers.8.self_attn_layer_norm', 'model.decoder.layers.8.self_attn.q_proj', 'model.decoder.layers.8.self_attn.k_proj', 'model.decoder.layers.8.self_attn.v_proj', 'model.decoder.layers.8.self_attn.out_proj', 'model.decoder.layers.8.self_attn', 'model.decoder.layers.9.self_attn_layer_norm', 'model.decoder.layers.9.self_attn.q_proj', 'model.decoder.layers.9.self_attn.k_proj', 'model.decoder.layers.9.self_attn.v_proj', 'model.decoder.layers.9.self_attn.out_proj', 'model.decoder.layers.9.self_attn', 'model.decoder.layers.10.self_attn_layer_norm', 'model.decoder.layers.10.self_attn.q_proj', 'model.decoder.layers.10.self_attn.k_proj', 'model.decoder.layers.10.self_attn.v_proj', 'model.decoder.layers.10.self_attn.out_proj', 'model.decoder.layers.10.self_attn', 'model.decoder.layers.11.self_attn_layer_norm', 'model.decoder.layers.11.self_attn.q_proj', 'model.decoder.layers.11.self_attn.k_proj', 'model.decoder.layers.11.self_attn.v_proj', 'model.decoder.layers.11.self_attn.out_proj', 'model.decoder.layers.11.self_attn', 'model.decoder.layers.12.self_attn_layer_norm', 'model.decoder.layers.12.self_attn.q_proj', 'model.decoder.layers.12.self_attn.k_proj', 'model.decoder.layers.12.self_attn.v_proj', 'model.decoder.layers.12.self_attn.out_proj', 'model.decoder.layers.12.self_attn', 'model.decoder.layers.13.self_attn_layer_norm', 'model.decoder.layers.13.self_attn.q_proj', 'model.decoder.layers.13.self_attn.k_proj', 'model.decoder.layers.13.self_attn.v_proj', 'model.decoder.layers.13.self_attn.out_proj', 'model.decoder.layers.13.self_attn', 'model.decoder.layers.14.self_attn_layer_norm', 'model.decoder.layers.14.self_attn.q_proj', 'model.decoder.layers.14.self_attn.k_proj', 'model.decoder.layers.14.self_attn.v_proj', 'model.decoder.layers.14.self_attn.out_proj', 'model.decoder.layers.14.self_attn', 'model.decoder.layers.15.self_attn_layer_norm', 'model.decoder.layers.15.self_attn.q_proj', 'model.decoder.layers.15.self_attn.k_proj', 'model.decoder.layers.15.self_attn.v_proj', 'model.decoder.layers.15.self_attn.out_proj', 'model.decoder.layers.15.self_attn', 'model.decoder.layers.16.self_attn_layer_norm', 'model.decoder.layers.16.self_attn.q_proj', 'model.decoder.layers.16.self_attn.k_proj', 'model.decoder.layers.16.self_attn.v_proj', 'model.decoder.layers.16.self_attn.out_proj', 'model.decoder.layers.16.self_attn', 'model.decoder.layers.17.self_attn_layer_norm', 'model.decoder.layers.17.self_attn.q_proj', 'model.decoder.layers.17.self_attn.k_proj', 'model.decoder.layers.17.self_attn.v_proj', 'model.decoder.layers.17.self_attn.out_proj', 'model.decoder.layers.17.self_attn', 'model.decoder.layers.18.self_attn_layer_norm', 'model.decoder.layers.18.self_attn.q_proj', 'model.decoder.layers.18.self_attn.k_proj', 'model.decoder.layers.18.self_attn.v_proj', 'model.decoder.layers.18.self_attn.out_proj', 'model.decoder.layers.18.self_attn', 'model.decoder.layers.19.self_attn_layer_norm', 'model.decoder.layers.19.self_attn.q_proj', 'model.decoder.layers.19.self_attn.k_proj', 'model.decoder.layers.19.self_attn.v_proj', 'model.decoder.layers.19.self_attn.out_proj', 'model.decoder.layers.19.self_attn', 'model.decoder.layers.20.self_attn_layer_norm', 'model.decoder.layers.20.self_attn.q_proj', 'model.decoder.layers.20.self_attn.k_proj', 'model.decoder.layers.20.self_attn.v_proj', 'model.decoder.layers.20.self_attn.out_proj', 'model.decoder.layers.20.self_attn', 'model.decoder.layers.21.self_attn_layer_norm', 'model.decoder.layers.21.self_attn.q_proj', 'model.decoder.layers.21.self_attn.k_proj', 'model.decoder.layers.21.self_attn.v_proj', 'model.decoder.layers.21.self_attn.out_proj', 'model.decoder.layers.21.self_attn', 'model.decoder.layers.22.self_attn_layer_norm', 'model.decoder.layers.22.self_attn.q_proj', 'model.decoder.layers.22.self_attn.k_proj', 'model.decoder.layers.22.self_attn.v_proj', 'model.decoder.layers.22.self_attn.out_proj', 'model.decoder.layers.22.self_attn', 'model.decoder.layers.23.self_attn_layer_norm', 'model.decoder.layers.23.self_attn.q_proj', 'model.decoder.layers.23.self_attn.k_proj', 'model.decoder.layers.23.self_attn.v_proj', 'model.decoder.layers.23.self_attn.out_proj', 'model.decoder.layers.23.self_attn']\n",
            "Baseline Model Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Model Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Model Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Model Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Attention0:  tensor(0.0317, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Attention23:  tensor(0.1329, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "TxjCKRfh9Rx2",
      "metadata": {
        "id": "TxjCKRfh9Rx2"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=10)\n",
        "progress_callback = WandbPredictionProgressCallback(trainer, processor.tokenizer, valid_dataset, 10,1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "babNyipE9TZv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "babNyipE9TZv",
        "outputId": "44d389db-b4d7-400a-ff37-51a78c18f341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxdf123\u001b[0m (\u001b[33mgct634\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240610_155640-dzmux8lh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription/runs/dzmux8lh' target=\"_blank\">whisper-medium-finedtuned-jamendo-snr-3-Kwangbin_testing</a></strong> to <a href='https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription' target=\"_blank\">https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription/runs/dzmux8lh' target=\"_blank\">https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription/runs/dzmux8lh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription/runs/dzmux8lh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7b0671bf25c0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "wandb.init(entity = \"gct634\", project = \"Noisy Lyrics Transcription\", name = OUTPUT_DIR\n",
        "           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fndcnOHFz-C0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "68be8ea85b8d4077adb430737bc919ee",
            "7213c882e7b04eba9ea7ad4e94151d76",
            "bd7f0867924d4661a376879fbb32c3c5",
            "22e8bf2fae1c40c5a08aa7837b53272d",
            "bdd677cde79448a0ae8fc0a8aba408f0",
            "2febe8c6b16b465694da4b9cf0d455a7",
            "20fe60e0eebd40d6b9649e71cd1de14f",
            "9cd56e8b7b334e3eb209f72ba4a65b57"
          ]
        },
        "id": "fndcnOHFz-C0",
        "outputId": "573c3270-b42d-4c99-8c65-3d7596d53f9f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING IN PROGRESS...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='133' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 133/1100 27:04 < 3:19:52, 0.08 it/s, Epoch 6/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.900900</td>\n",
              "      <td>1.302378</td>\n",
              "      <td>59.123758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.999000</td>\n",
              "      <td>1.036440</td>\n",
              "      <td>56.413731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.568300</td>\n",
              "      <td>0.631665</td>\n",
              "      <td>64.588979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.281200</td>\n",
              "      <td>0.641351</td>\n",
              "      <td>42.818428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.225100</td>\n",
              "      <td>0.669363</td>\n",
              "      <td>52.574526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.190800</td>\n",
              "      <td>0.649837</td>\n",
              "      <td>87.669377</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2017, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.1171875\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2010, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2006, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2001, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2003, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2006, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='418' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 418/1100 1:32:12 < 2:31:10, 0.08 it/s, Epoch 19/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.900900</td>\n",
              "      <td>1.302378</td>\n",
              "      <td>59.123758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.999000</td>\n",
              "      <td>1.036440</td>\n",
              "      <td>56.413731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.568300</td>\n",
              "      <td>0.631665</td>\n",
              "      <td>64.588979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.281200</td>\n",
              "      <td>0.641351</td>\n",
              "      <td>42.818428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.225100</td>\n",
              "      <td>0.669363</td>\n",
              "      <td>52.574526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.190800</td>\n",
              "      <td>0.649837</td>\n",
              "      <td>87.669377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.134500</td>\n",
              "      <td>0.756346</td>\n",
              "      <td>48.644986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.145900</td>\n",
              "      <td>0.604894</td>\n",
              "      <td>44.173442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.122000</td>\n",
              "      <td>0.607589</td>\n",
              "      <td>33.107498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.630334</td>\n",
              "      <td>44.941283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.089500</td>\n",
              "      <td>0.643012</td>\n",
              "      <td>40.514905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.076400</td>\n",
              "      <td>0.624659</td>\n",
              "      <td>57.588076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.061200</td>\n",
              "      <td>0.630985</td>\n",
              "      <td>39.747064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.059600</td>\n",
              "      <td>0.638645</td>\n",
              "      <td>36.675700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.059400</td>\n",
              "      <td>0.680554</td>\n",
              "      <td>49.728997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.055100</td>\n",
              "      <td>0.703426</td>\n",
              "      <td>40.289070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.045800</td>\n",
              "      <td>0.648394</td>\n",
              "      <td>35.636856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.044800</td>\n",
              "      <td>0.666450</td>\n",
              "      <td>44.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.050200</td>\n",
              "      <td>0.675371</td>\n",
              "      <td>45.573622</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2010, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2010, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2008, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2006, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2009, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2008, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2010, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2009, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2004, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2004, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2004, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2003, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Convolutional Entropy (With Noise): 14.889142990112305\n",
            "Baseline Convolutional Entropy (Without Noise): 14.897878646850586\n",
            "Baseline Convolutional Entropy 2 (With Noise): 14.091837882995605\n",
            "Baseline Convolutional Entropy 2 (Without Noise): 14.116389274597168\n",
            "Baseline Mean Squared Difference in Conv1:  tensor(0.0339, device='cuda:0')\n",
            "Baseline Mean Squared Difference in Conv2:  tensor(0.2013, device='cuda:0')\n",
            "Fine Tuned Convolutional Entropy (With Noise): 14.8828125\n",
            "Fine Tuned Convolutional Entropy (Without Noise): 14.8984375\n",
            "Fine Tuned Convolutional Entropy 2 (With Noise): 14.09375\n",
            "Fine Tuned Convolutional Entropy 2 (Without Noise): 14.125\n",
            "Fine-Tuned Mean Squared Difference in Conv1:  tensor(0.0340, device='cuda:0', dtype=torch.float16)\n",
            "Fine-Tuned Mean Squared Difference in Conv2:  tensor(0.2004, device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE TRAINING\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='2914.158 MB of 2914.158 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68be8ea85b8d4077adb430737bc919ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MSD Attention Layer0 </td><td>▇▇▆▆▆▆██▃▁▁▁▃▃▂▂▃▃▃</td></tr><tr><td>MSD Attention Layer23 </td><td>▁▄▇█▆▅▆▆▅▅▄▃▃▃▄▄▄▃▄</td></tr><tr><td>MSD_conv1</td><td>▁▁▃▃▃▆█▆▆▆▆▆██▆████</td></tr><tr><td>MSD_conv2</td><td>█▅▃▁▂▃▅▅▄▃▅▄▅▅▃▃▃▂▃</td></tr><tr><td>entropy_conv1_with_noise</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>entropy_conv1_without_noise</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>entropy_conv2_with_noise</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>entropy_conv2_without_noise</td><td>█▁█████████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>eval/loss</td><td>███▅▅▁▁▁▁▂▂▁▁▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▁▂▂▂▂</td></tr><tr><td>eval/runtime</td><td>▅▅▅▄▄▅▅▂▂▄▄██▂▂▃▃▁▁▃▃▂▂▅▅▅▂▂▂▂▄▄▃▃▁▁▂▂▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▃▃▃▅▅▃▃▆▆▄▄▁▁▆▆▆▆██▆▆▆▆▄▄▄▆▆▇▇▅▅▆▆██▆▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▃▃▃▅▅▃▃▆▆▅▅▁▁▆▆▆▆██▆▆▆▆▄▄▄▆▆▇▇▅▅▆▆██▆▆▆▆</td></tr><tr><td>eval/wer</td><td>▄▄▄▄▄▅▅▂▂▃▃██▃▃▂▂▁▁▃▃▂▂▄▄▄▂▂▁▁▃▃▂▂▁▁▂▂▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>███▅▅▄▄▅▅▃▃▂▂▃▃▃▃▃▃▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▅▅</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>███▅▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MSD Attention Layer0 </td><td>0.03162</td></tr><tr><td>MSD Attention Layer23 </td><td>0.1449</td></tr><tr><td>MSD_conv1</td><td>0.034</td></tr><tr><td>MSD_conv2</td><td>0.20044</td></tr><tr><td>entropy_conv1_with_noise</td><td>14.88281</td></tr><tr><td>entropy_conv1_without_noise</td><td>14.89844</td></tr><tr><td>entropy_conv2_with_noise</td><td>14.09375</td></tr><tr><td>entropy_conv2_without_noise</td><td>14.125</td></tr><tr><td>epoch</td><td>19.0</td></tr><tr><td>eval/loss</td><td>0.67537</td></tr><tr><td>eval/runtime</td><td>75.7721</td></tr><tr><td>eval/samples_per_second</td><td>1.148</td></tr><tr><td>eval/steps_per_second</td><td>0.238</td></tr><tr><td>eval/wer</td><td>45.57362</td></tr><tr><td>total_flos</td><td>1.349647929704448e+19</td></tr><tr><td>train/epoch</td><td>19.0</td></tr><tr><td>train/global_step</td><td>418</td></tr><tr><td>train/grad_norm</td><td>6.13747</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0502</td></tr><tr><td>train_loss</td><td>0.27367</td></tr><tr><td>train_runtime</td><td>5511.8869</td></tr><tr><td>train_samples_per_second</td><td>6.314</td></tr><tr><td>train_steps_per_second</td><td>0.2</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">whisper-medium-finedtuned-jamendo-snr-3-Kwangbin_testing</strong> at: <a href='https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription/runs/dzmux8lh' target=\"_blank\">https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription/runs/dzmux8lh</a><br/> View project at: <a href='https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription' target=\"_blank\">https://wandb.ai/gct634/Noisy%20Lyrics%20Transcription</a><br/>Synced 5 W&B file(s), 19 media file(s), 24 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240610_155640-dzmux8lh/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# processor.save_pretrained(training_args.output_dir)\n",
        "\n",
        "trainer.add_callback(progress_callback)\n",
        "trainer.add_callback(early_stopping)\n",
        "trainer.add_callback(noisy_eval)\n",
        "print('TRAINING IN PROGRESS...')\n",
        "trainer.train()\n",
        "print('DONE TRAINING')\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKiSssHSmTsr"
      },
      "id": "aKiSssHSmTsr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "68be8ea85b8d4077adb430737bc919ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7213c882e7b04eba9ea7ad4e94151d76",
              "IPY_MODEL_bd7f0867924d4661a376879fbb32c3c5"
            ],
            "layout": "IPY_MODEL_22e8bf2fae1c40c5a08aa7837b53272d"
          }
        },
        "7213c882e7b04eba9ea7ad4e94151d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdd677cde79448a0ae8fc0a8aba408f0",
            "placeholder": "​",
            "style": "IPY_MODEL_2febe8c6b16b465694da4b9cf0d455a7",
            "value": "2914.226 MB of 2914.226 MB uploaded\r"
          }
        },
        "bd7f0867924d4661a376879fbb32c3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20fe60e0eebd40d6b9649e71cd1de14f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cd56e8b7b334e3eb209f72ba4a65b57",
            "value": 1
          }
        },
        "22e8bf2fae1c40c5a08aa7837b53272d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd677cde79448a0ae8fc0a8aba408f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2febe8c6b16b465694da4b9cf0d455a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20fe60e0eebd40d6b9649e71cd1de14f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cd56e8b7b334e3eb209f72ba4a65b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}